GOAL : Design a distributed key value caching system, like Memcached or Redis.

1. FEATURES:
	1. Let's assume we are looking to cache on the scale of Google or Twitter. The total size of the cache would be a few TBs.
	2. It is possible that we might get entries when we would not have space to accommodate new entries. In such cases, we would need 
	   to remove one or more entries to make space for the new entry.
	3. There are majorly three kinds of caching systems.
		a. Write through cache : This is a caching system where writes go through the cache and write is confirmed as success only if writes to DB and the
		   cache BOTH succeed. This is really useful for applications which write and re-read the information quickly. However, write latency will be 
		   higher in this case as there are writes to 2 separate systems
		b. Write around cache : This is a caching system where write directly goes to the DB. The cache system reads the information from DB incase of a miss. 
		   While this ensures lower write load to the cache and faster writes, this can lead to higher read latency incase of applications which write and 
		   re-read the information quickly.
		c. Write back cache : This is a caching system where the write is directly done to the caching layer and the write is confirmed as soon as the 
		   write to the cache completes. The cache then asynchronously syncs this write to the DB. This would lead to a really quick write latency and 
		   high write throughput. But, as is the case with any non-persistent / in-memory write, we stand the risk of losing the data incase the caching 
		   layer dies. We can improve our odds by introducing having more than one replica acknowledging the write ( so that we don’t lose data if just 
		   one of the replica dies )

2. ESTIMATION:
	say we have 30 TB data and 10M QPS. Suppose every server has capacity of 64GB RAM and 4 core CPUs
	so we need 30TB/64GB = ~ 469 servers.
	10M QPS => ~21322 QPS per server
	4 core => 21322/4 = ~ 5331 QPS per core.
		say one request takes ~ 0.19 milliseconds
	
	bandwidth : say outgoing data size for one request is 1KB.
	21322 QPS => 21322 KBPS => 21 MBPS

3. DESIGN GOAL:
	Latency must be as low as possible.
	